{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasagne Seed for Reproducibility\n",
    "lasagne.random.set_rng(np.random.RandomState(1))\n",
    "\n",
    "# Sequence Length\n",
    "SEQ_LENGTH = 64\n",
    "\n",
    "# Number of units in the two hidden (LSTM) layers\n",
    "N_HIDDEN = 512\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 1\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [\"Data/irish_music.txt\", \"Data/swedish_music.txt\",\"Data/french_music.txt\"]\n",
    "context = [\"irish\",\"swedish\",\"french\"]\n",
    "in_text=[0]*len(files)\n",
    "for i in range(0,len(files)) :\n",
    "    try:\n",
    "        #You can also use your own file\n",
    "        #The file must be a simple text file.\n",
    "        #Simply edit the file name below and uncomment the line.\n",
    "        #in_text = open('data.txt').read() #nietzsche text\n",
    "        in_text[i] = open(files[i], 'r').read() #Music file\n",
    "        in_text[i]= in_text[i].replace('  ','')\n",
    "        in_text[i]= in_text[i].replace('\\r','')\n",
    "        in_text[i]=''.join([j if ord(j) < 128 else ' ' for j in in_text[i]])\n",
    "        in_text[i] = in_text[i].decode(\"utf-8-sig\").encode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Please verify the location of the input file/URL.\")\n",
    "        print(\"A sample txt file can be downloaded from https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "        raise IOError('Unable to Read Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generation_phrase = \"g2B2|d2d2 d2e2|d2cB g2fg|a2A2 A2\\nGA|B2B2 cBAG|A2B2 g2fg|a2gf g2fe|d2B2 B2:|\\n<end>\\n<start>\\n\"\n",
    "#generation_phrase=\"The quick brown fox jumps\"\n",
    "generation_phrases=[]\n",
    "generation_phrases.append(\"a2 ba ge | d2 ed cB | c2 cd Bc | A4 AB |\\ncB cd ef | g4 fe | d2 dB AG | E4 Bd |\\ne2 ge dB | A2 B/c/d D2 | E2 GA/B/ A>G | G4 || \\n<end>\\n<start>\\n\")\n",
    "generation_phrases.append(\"|:A2df a2a2 g4|A2^ce gage f3f|fafd ege^c d3e|f2ed ^c2ec A4|\\nA2df a2a2 g4|A2^ce gage f3f|fafd ege^c d3e|f2ed ^c2ec d4:|\\n<end>\\n<start>\\n\")\n",
    "generation_phrases.append(\"g2gb g2gb g2gb | f2fa f2fa f2fa | f2ed ^cde^c A4 :|\\n|: A2d2 ^cde^c d4 | A2f2 efge f4 | A2d2 ^cde^c d4 |\\nA2cA G2BG A4 | A2cA G2BG F2AF | EFE^C D8 :| \\n<end>\\n<start>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=set()\n",
    "for text in in_text:\n",
    "        a = a|set(text)\n",
    "chars = list(a)\n",
    "chars.sort()\n",
    "data_size, vocab_size = sum(len(text) for text in in_text), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "vocab_size = vocab_size+3\n",
    "n = len(chars)\n",
    "char_to_ix['irish']=n\n",
    "char_to_ix['swedish']=n+1\n",
    "char_to_ix['french']=n+2\n",
    "ix_to_char[n]='irish'\n",
    "ix_to_char[n+1]='swedish' \n",
    "ix_to_char[n+2]='french' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(p, data,context, batch_size = BATCH_SIZE, return_target=True):\n",
    "    '''\n",
    "    This function produces a semi-redundant batch of training samples from the location 'p' in the provided string (data).\n",
    "    For instance, assuming SEQ_LENGTH = 5 and p=0, the function would create batches of \n",
    "    5 characters of the string (starting from the 0th character and stepping by 1 for each semi-redundant batch)\n",
    "    as the input and the next character as the target.\n",
    "    To make this clear, let us look at a concrete example. Assume that SEQ_LENGTH = 5, p = 0 and BATCH_SIZE = 2\n",
    "    If the input string was \"The quick brown fox jumps over the lazy dog.\",\n",
    "    For the first data point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'T','h','e',' ','q'\n",
    "    y (the targets of the neural network) would be the encoding of 'u'\n",
    "    For the second point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'h','e',' ','q', 'u'\n",
    "    y (the targets of the neural network) would be the encoding of 'i'\n",
    "    The data points are then stacked (into a three-dimensional tensor of size (batch_size,SEQ_LENGTH,vocab_size))\n",
    "    and returned. \n",
    "    Notice that there is overlap of characters between the batches (hence the name, semi-redundant batch).\n",
    "    '''\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "            x[n,i,char_to_ix[context]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "from itertools import count\n",
    "\n",
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "    counter = count()\n",
    "   \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, None, vocab_size))\n",
    "\n",
    "    # We now build the LSTM layer which takes l_in as the input layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "\n",
    "    l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    l_forward_1_drop = lasagne.layers.DropoutLayer(l_forward_1, p=0.1)\n",
    "    \n",
    "    l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "        l_forward_1_drop, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    # The l_forward layer creates an output of dimension (batch_size, SEQ_LENGTH, N_HIDDEN)\n",
    "    # Since we are only interested in the final prediction, we isolate that quantity and feed it to the next layer. \n",
    "    # The output of the sliced layer will then be of size (batch_size, N_HIDDEN)\n",
    "    l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)\n",
    "\n",
    "    # The sliced output is then passed through the softmax nonlinearity to create probability distribution of the prediction\n",
    "    # The output of this stage is (batch_size, vocab_size)\n",
    "    l_out = lasagne.layers.DenseLayer(l_forward_slice, num_units=vocab_size, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Theano tensor for the targets\n",
    "    target_values = T.ivector('target_output')\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "    cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()\n",
    "\n",
    "    # Retrieve all parameters from the network\n",
    "    params = pickle.load(open(\"params_context_country_3\",\"rb\"))\n",
    "    lasagne.layers.set_all_param_values(l_out,params)\n",
    "    all_params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "\n",
    "    # Compute AdaGrad updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "    # In order to generate text from the network, we need the probability distribution of the next character given\n",
    "    # the state of the network and the input (a seed).\n",
    "    # In order to produce the probability distribution of the prediction, we compile a function called probs. \n",
    "    \n",
    "    probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)\n",
    "\n",
    "    # The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    # The phrase is set using the variable generation_phrase.\n",
    "    # The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "    visualize = theano.function([l_in.input_var], lasagne.layers.get_output( l_forward_slice), allow_input_downcast=True)\n",
    "    \n",
    "    def try_it_out(context, generation_phrase, N=600):\n",
    "        '''\n",
    "        This function uses the user-provided string \"generation_phrase\" and current state of the RNN generate text.\n",
    "        The function works in three steps:\n",
    "        1. It converts the string set in \"generation_phrase\" (which must be over SEQ_LENGTH characters long) \n",
    "           to encoded format. We use the gen_data function for this. By providing the string and asking for a single batch,\n",
    "           we are converting the first SEQ_LENGTH characters into encoded form. \n",
    "        2. We then use the LSTM to predict the next character and store it in a (dynamic) list sample_ix. This is done by using the 'probs'\n",
    "           function which was compiled above. Simply put, given the output, we compute the probabilities of the target and pick the one \n",
    "           with the highest predicted probability. \n",
    "        3. Once this character has been predicted, we construct a new sequence using all but first characters of the \n",
    "           provided string and the predicted character. This sequence is then used to generate yet another character.\n",
    "           This process continues for \"N\" characters. \n",
    "        To make this clear, let us again look at a concrete example. \n",
    "        Assume that SEQ_LENGTH = 5 and generation_phrase = \"The quick brown fox jumps\". \n",
    "        We initially encode the first 5 characters ('T','h','e',' ','q'). The next character is then predicted (as explained in step 2). \n",
    "        Assume that this character was 'J'. We then construct a new sequence using the last 4 (=SEQ_LENGTH-1) characters of the previous\n",
    "        sequence ('h','e',' ','q') , and the predicted letter 'J'. This new sequence is then used to compute the next character and \n",
    "        the process continues.\n",
    "        '''\n",
    "\n",
    "        assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "        sample_ix = []\n",
    "        x,_ = gen_data(len(generation_phrase)-SEQ_LENGTH,generation_phrase,context,1,0)\n",
    "        vis=[]\n",
    "        inputs=\"\"+generation_phrase[len(generation_phrase)-1]\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            ix = np.argmax(probs(x).ravel())\n",
    "            op=visualize(x)\n",
    "            vis.append([op[0,kk] for kk in range(512)])\n",
    "            inputs+=ix_to_char[ix]\n",
    "            # Alternatively, to sample from the distribution instead:\n",
    "            # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "            sample_ix.append(ix)\n",
    "            x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "            x[:,SEQ_LENGTH-1,:] = 0\n",
    "            x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "        vis=numpy.array(vis)\n",
    "        index=str(next(counter))\n",
    "        pickle.dump(vis,open(\"visualize/output\"+index,'wb'))\n",
    "        fname=open('visualize/input'+index+'.txt','w')\n",
    "        fname.write(inputs)\n",
    "        fname.close()\n",
    "\n",
    "        random_snippet = ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "        print(\"----\\n %s \\n %s \\n----\" % (context,random_snippet))\n",
    "        fname=open('Output/country_lstm_output_context_dropout_'+context+'.txt','a')\n",
    "        fname.write(\"----\\n<start>\\n%s\\n----\" % (random_snippet))\n",
    "        fname.close()\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    #print(\"Seed used for text generation is: \" + generation_phrase)\n",
    "    p = 0\n",
    "    try:\n",
    "        pos=0\n",
    "        positions=[0]*len(files)\n",
    "        for it in xrange(data_size * num_epochs / BATCH_SIZE):\n",
    "            try_it_out(context[0],generation_phrases[0],N=600)\n",
    "            try_it_out(context[1],generation_phrases[1],N=600)\n",
    "            try_it_out(context[2],generation_phrases[2],N=600)# Generate text using the p^th character as the start. \n",
    "            \n",
    "            avg_cost = 0;\n",
    "            for _ in range(PRINT_FREQ):\n",
    "                p=positions[pos]\n",
    "                x,y = gen_data(p,in_text[pos],context[pos])\n",
    "                \n",
    "                #print(p)\n",
    "                p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "                if(p+BATCH_SIZE+SEQ_LENGTH >= len(in_text[pos])):\n",
    "                    print('Carriage Return')\n",
    "                    p = 0;\n",
    "                \n",
    "                #print(\"context is \",context[pos],\"positions is \", p)\n",
    "                avg_cost += train(x, y)\n",
    "                positions[pos]=p\n",
    "                pos = (pos+1)%len(files)\n",
    "            all_param = lasagne.layers.get_all_param_values(l_out)\n",
    "            #  pickle.dump(all_param,open(\"params_context_country_playlist\",'wb'))\n",
    "            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname=open('Output/country_lstm_loss_context_dropout_web.txt','a')\n",
    "            fname.write(\"\\n{},{}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname.close()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "----\n",
      " irish \n",
      " X:22\n",
      "T:Bourneel Bollon Boung\n",
      "R:schontis\n",
      "C:Trad Polla de cantes tmer (ilast Simon (1670-1933))\n",
      "H:Antrotander in thaettess tute.\n",
      "Z:iXXX-27i\n",
      "Z:id:hn-schontiis-2\n",
      "M:C|\n",
      "K:Dm\n",
      "A2 A>A B>cA|GE B>c d>e|fe ff/e/|df f>f|d>f fd c>d|Bc Bc|df gf ed|(3fec d2:|\n",
      "<end>\n",
      "<start>\n",
      "X:12\n",
      "T:La Reine da Laithi\n",
      "R:cortlan\n",
      "C:Tumalis tre ond\n",
      "R:sl\\\"angeolska\n",
      "C:Trad (hllsding, (1815-1980)\n",
      "H:Joh brain in \"umorie Jn'an Sellem the mert Frorch\n",
      "R:slig\\\"ondes\n",
      "Z:id:hn-sohk2-ie-s2-\n",
      "M:C|\n",
      "K:D\n",
      "A2 |D FA dc AF|G3 E2 E2|cA B2 A2|c2 A2 cc|BA A2 G2|A4 A2|\n",
      "B2 Bc B2|dc BA d2|e2 f2 fe|fe d2 f2|\n",
      "e2 de fa|f2 d2 fe|de d2 ef|g2 fd fd|ef ed ^cd|1 B2  \n",
      "----\n",
      "----\n",
      " swedish \n",
      " X:10\n",
      "T:Locarilla (1922))\n",
      "O:France\n",
      "Z:ho-schottin\n",
      "C:Martin La Lincsouvi\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:G\n",
      "ef|g2 de fe|de ed Bc|d2 de fe|df gf ee|df ef gf|ed ef ge|\n",
      "fe fe fe|dc BA G/A/F/A/|G2 GB GA|B2 A2 :|\n",
      "<end>\n",
      "<start>\n",
      "X:10\n",
      "T:La bainain\n",
      "C:Palar Meligo\n",
      "O:Franca\n",
      "Z:id:hn-schottis-29\n",
      "M:C|\n",
      "L:1/8\n",
      "K:G\n",
      "e2d2 d2f2|e2d2 c2|A2 G2 A2|^G2G A2|B2 B2 A2|\n",
      "A2 A2 B2|c2 A2 Bc|d2 A2 G2|BA G2 B2|A4:|\n",
      "<end>\n",
      "<start>\n",
      "X:12\n",
      "T:Las deacde denna\n",
      "D:Danar Aurains Leath\n",
      "R:har danel\n",
      "Z:id:hn-sc-24\n",
      "M:6/8\n",
      "L:1/86\n",
      "K:G\n",
      "BABG G2BG ABAG|B2A2 B2AG|A2A2 AF|GFGF G2A2|\n",
      "c2Bc d2B2|G2G2 G2B2|c2c2 A2G2|B2B2 G2|B2B2 G2:|\n",
      "|:f2|d2c2 d2f2|f2f2 f2f2|e2f2 f2f2|e2e2 e2 \n",
      "----\n",
      "----\n",
      " french \n",
      " X: 16\n",
      "T:Scottish ( Larnan aigs\n",
      "R:scho\n",
      "C:Pers Brann onde Pilo Manch\n",
      "Z:id:hn-s--18\n",
      "Y:Co4\n",
      "L:1/8\n",
      "K:G\n",
      "e2 | f2 dd cd|ef gf ef|ef ed ef|ed Bd cB|A2 A2 Af|fe df ed|1 Bc de f2:|2 f2 df ||\n",
      "P:Varsanion oletl\n",
      "|:GA G2 BG|de f2 gf|ef ed cd|ec ce dc|\n",
      "fe fd cd|1 d2 fd cA|B2 d2 G2|B2 A2 :|2 f2 af|g2 ed |\n",
      "c2 cA | B2 GA GB|A2 G2 GA|BG EF G2|1 A2 AG :|2 A2 A2|B2 AG|F2 G2|A>B GA|G2 G2:|\n",
      "|:A2 A>c|d2 f>d|d>c de|d2 fe|df df|dc de|d2 dd|\n",
      "dc Ac|dc BA|d2 e2|fe dc|d2 ed|dc de|dc de|fd dc|dB cd|1 d2 dc:|2 A2:|\n",
      "<end>\n",
      "<start>\n",
      "X:11\n",
      "T:Polka Plla du Samen\n",
      "R:slin (3\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:G\n",
      "A2 AB/c/ d2|c2 Bc/d/|ef gf ef|d2 cB AG|F2 A2 B \n",
      "----\n",
      "Epoch 0.0 average loss = 0.640120446682\n",
      "----\n",
      " irish \n",
      " obs 'ol\n",
      "'s'\\\n",
      "o'm\n",
      "o'm\n",
      "''s \\''l''i\\''s lile'sellimeececelececlecebeeleellellelele\n",
      "e:|leele\n",
      "fellelee\n",
      "cellecllestteleeeblelestellelelestlllettees\n",
      "celellefellele,\n",
      "ceslelelelelleletolellelebellecetelelleelleelecellclecletteleleelellomelleelefll\\\"oteleslellellellelllceettlecestellcecllecllelecelelellecl\\\"olemlelssellelelelelelleclellslcellecslleliliteelttescemleelleceleceseleeeelelbelemellellleeteelecleleleleleeteeleleleleteece\n",
      "ceelem\n",
      "esteelleeleleleemeec\n",
      "ceelecelielleseleleleellecleclec\n",
      "eceleeceleleclelellee\n",
      "eelleetelleceltelelellestelleteteellelleelleelellelllel,llelltetleetellelelecececeseegbeemem \n",
      "----\n",
      "----\n",
      " swedish \n",
      " ssttellleltttsescesllellleetellecesllleclelellelleelellletteelelllettelemelellecleteeelelleeleleelleltellelleellelemelelellletteelleelcellelelelestettelleellelleellelllttetellelleellellelemellecllellellleelleclecetleelelleelleltettelebellellellelflelleellelllelteellemtcelleleletelleltellellllclectelellellelettepelel\\\"olelteleelellemeleeteeeelettelelilelelellellecleepelleclelleclelestelleebe\\geleel,stellestellelletlleclleeleceeleeceeleleeceleletelbellescellecleltelle\n",
      "fellelleelleltellellles\n",
      "selleellelelemmelecels\\\\eelle\n",
      "elsleteftelbeeleemeelelelelelte\n",
      "Reslillleelele.\n",
      "ecselleclteeleemlelellelele \n",
      "----\n",
      "----\n",
      " french \n",
      " t3\n",
      "b3e\n",
      "g2gegagaggecagegeegeeagageggeegga\\eag\n",
      "f2gee,\n",
      "geleelelelelllelleltellellelllelleeleeleeleelle\n",
      "ceelelleel\n",
      "feleelleestelletteelelbellecleltellellellecltcelellelelellelleelelelleolleleclelellleelelleellelecellelecelleeleeleclllellestemlellestleleleneleleelelllemeeteelc\\t\\\"oleelleleteleececlllefte\\bellelllelleclecellecleelecleeleleleltlecleele\n",
      "celeelel\n",
      "feleleleltellecleleeclellelteet\\eelleelleesce\n",
      "ceelelelemeleellecll\\\"ome,\n",
      "beslelelteeleleltee\n",
      "celemesceleblebbelelceceleselemettele\n",
      "feeleleeleletellettel\n",
      "B:tell\\\"olelsellecstellelleleetlecelebelclecsselellelb\n",
      "eelelleellelleetteeblelt\n",
      "eeteelelte \n",
      "----\n",
      "Epoch 0.000754824908632 average loss = 6.54923248291\n",
      "----\n",
      " irish \n",
      " B3AAA A3:|| A A3:| A3 A A2A2| A3 A3|A3 A A2A|A A A3| A3 | A3 A3:| A3 AAAA|A3A A A A3|AAA A3:| A3 A A3|A A3A A3 A|A3 A2A A2| A3A A3 A3|AAA A3:| A3 A A3| A3 A3|A3 A3|A A A2|A A B/A3:| A3 A |A3 A3:|:|:  A A A3| A3 A3A A3| A2AA A2AA|A A A2:| A3  A A3A|AAA AA A4|A A3 A3| A3 A A3:| A3 A A2AA|A A A2A |AA3 A3:| A3 A A3|AAA A2:| A3 A3:| A3 A A A3A A2A|A A2A A3| A3 A A3|AA A A2AA|A A2A A|AAAAA A2||A A3 A3|A2A A2A|AA A3| A3 A|A2A A2:| A A A3|AA A3 AA|A A3| A4 || A3 A A2A2AABA A3 | A2 A3: | A2 A3|AA A3|A A A3:| A2 A| A3 A A2A2A A2A | A3A A3 A3| A3 A3 A3|A A A A3|AAA A|AAA A|ABA A3:|| A2 A A2A A3:| A2A A3| \n",
      "----\n",
      "----\n",
      " swedish \n",
      " B4:|AAA A3| A3A A3|A3A A A2A A|A3 A A3:| A A A3|AA A3 A|AAA A3|| A3 A A|A2 A3:| A3 A A3|A2A A A3A || A2A A3| A A2A A2 || A3 A A3AA| A2A A3A A2|A A3A A3| A3 A3| A A A3A|A3 A A2|A2A A | A3 | A3 | A2 |AAA A3:| |  A A A3:| A3 A A3|A4 A A3|A A3A A3:| | A3  A|AAA A |A2A A3:|| A A A3:| A3 A3A A2A A3:|A A3 A3|A A A A3| A3 A A3|AAA A3:| A3 A A2A2A|A3 A A3|A3 A A3| A3A A3:| A2 A3:| A A A3:| A3 A A3| A3 A3| A3 A A3|A3A A2A |A A2A :| A3 A A3 | A2 A3:| A2 A A3|AA A3:| A3 A A2A|A3| A3 | A3 A | A2 A A3:| A3 A3 ||A A A3|A A A4:|| A A3:|\n",
      "| AA A3 A3 A3:| 3 A A3:||| A A | A2A A A2A3 | A2A A3|:| A A2A A2A2 A2A A3 \n",
      "----\n",
      "----\n",
      " french \n",
      " g3|:A4 :|\n",
      "B3 A3:|A A A A A|AA A A3| A3 A A3:| A3  A3| A3 AA| A A| A3 | A3 | A3 A A2A|A3 A3 || A3 A3 || A A A3:||  | A A2A A3:| AA A3 A3|A2A A2A A|A4 A3|A A4 ||A2 A A2A|A3 A4| A3 A3:| A3 A A3|AA A A3:| A3 A3 |A3 A3 AAA2A B2A | A3 A|A2A A2A A3| A3 A A3:|| A A A A2A2A2 A3:| A3 AAA2A A3|AA A3:|| | A A |2A3 A3:|| A A A2A|B2A AA A3:| A|A A2 A|A4 A4|| A A A AAA|A2AA A4| A3 |:| A3 A A3||A A2A A || A A A3| A3 A A3:| A2 A2 A4|AA A3:| A3 A | A3 A3| A2A A3:|A3 A A3:|| A A A3:|  A3 A A3|AAA A3|A3  A|A A A|AAA A|:| A A3 A|A2A A2 A3| A2A A3 A3|AA A A2A A|A A A2A A3|AAA A3 | A3 AAA|A3 A A3|AA A A3|AA A A3|A3  \n",
      "----\n",
      "Epoch 0.00150964981726 average loss = 7.39498233795\n",
      "----\n",
      " irish \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-57a991688055>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mtry_it_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgeneration_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mtry_it_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgeneration_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mtry_it_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgeneration_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Generate text using the p^th character as the start.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-57a991688055>\u001b[0m in \u001b[0;36mtry_it_out\u001b[1;34m(context, generation_phrase, N)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;31m# Pick the character that got assigned the highest probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0mvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
