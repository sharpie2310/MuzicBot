{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled, CuDNN not available)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #You can also use your own file\n",
    "    #The file must be a simple text file.\n",
    "    #Simply edit the file name below and uncomment the line.\n",
    "    #in_text = open('data.txt').read() #nietzsche text\n",
    "    in_text = open('Data/data_new.txt', 'r').read() #Music file\n",
    "    in_text=in_text.replace('  ','')\n",
    "    in_text=''.join([i if ord(i) < 128 else ' ' for i in in_text])\n",
    "    in_text = in_text.decode(\"utf-8-sig\").encode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(\"Please verify the location of the input file/URL.\")\n",
    "    print(\"A sample txt file can be downloaded from https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "    raise IOError('Unable to Read Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generation_phrase = \"g2B2|d2d2 d2e2|d2cB g2fg|a2A2 A2\\nGA|B2B2 cBAG|A2B2 g2fg|a2gf g2fe|d2B2 B2:|\\n<end>\\n<start>\\n\"\n",
    "#generation_phrase=\"The quick brown fox jumps\"\n",
    "#generation_phrase =\"TFED|A2AA2 \\\\nd | FGFTFED|B2BBcd| cBAdAF|ABAA2::\\nd |TcBA cec|dBB TB2d|TcBAcec|ABAA2 \\\\nd |TcBA cec|dBBB2d|TcBAdAF|A>BA A2::\\nG |TF2dTF2d|B2BBcd|TF2d TF2d|A2AA2 \\\\nG |TF2dTF2d|B2BBcd|TcBAdAF|ABAA2:|\\n<end>\\n<start>\\nX:0590\\nT:The Lillies of France.\\nM:C\\nL:1/8\\nQ:1/4=120\\nI: :: |:\\nZ:Jack Campin * www.campin.me.uk * 2009\\nK:D\\n(A3/B//c//)|d2 A>F D2G>A|B<GE>d {d}Tc3 d|ec2edB2d|{cd}e2 E>^G A2 ::\\nAG |F>GA=c BGGF |G>ABd {d}c2BA|d2 d>dd2cB| (BA) (GF) {F}E2A2|\\nDF`A=c BGGF |E2 ed {d}c2BA|d2(e3/f//g//) fddA|BG```FE D2 |]\\n<end>\\n<start>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = list(set(in_text))\n",
    "data_size, vocab_size = len(in_text), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lasagne Seed for Reproducibility\n",
    "lasagne.random.set_rng(np.random.RandomState(1))\n",
    "\n",
    "# Sequence Length\n",
    "SEQ_LENGTH = 64\n",
    "\n",
    "# Number of units in the two hidden (LSTM) layers\n",
    "N_HIDDEN = 256\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 500\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(p, batch_size = BATCH_SIZE, data=in_text, return_target=True):\n",
    "    '''\n",
    "    This function produces a semi-redundant batch of training samples from the location 'p' in the provided string (data).\n",
    "    For instance, assuming SEQ_LENGTH = 5 and p=0, the function would create batches of \n",
    "    5 characters of the string (starting from the 0th character and stepping by 1 for each semi-redundant batch)\n",
    "    as the input and the next character as the target.\n",
    "    To make this clear, let us look at a concrete example. Assume that SEQ_LENGTH = 5, p = 0 and BATCH_SIZE = 2\n",
    "    If the input string was \"The quick brown fox jumps over the lazy dog.\",\n",
    "    For the first data point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'T','h','e',' ','q'\n",
    "    y (the targets of the neural network) would be the encoding of 'u'\n",
    "    For the second point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'h','e',' ','q', 'u'\n",
    "    y (the targets of the neural network) would be the encoding of 'i'\n",
    "    The data points are then stacked (into a three-dimensional tensor of size (batch_size,SEQ_LENGTH,vocab_size))\n",
    "    and returned. \n",
    "    Notice that there is overlap of characters between the batches (hence the name, semi-redundant batch).\n",
    "    '''\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "   \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, None, vocab_size))\n",
    "\n",
    "    # We now build the LSTM layer which takes l_in as the input layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "\n",
    "    l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    l_forward_1_drop = lasagne.layers.DropoutLayer(l_forward_1, p=0.1)\n",
    "    \n",
    "    l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "        l_forward_1_drop, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    # The l_forward layer creates an output of dimension (batch_size, SEQ_LENGTH, N_HIDDEN)\n",
    "    # Since we are only interested in the final prediction, we isolate that quantity and feed it to the next layer. \n",
    "    # The output of the sliced layer will then be of size (batch_size, N_HIDDEN)\n",
    "    l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)\n",
    "\n",
    "    # The sliced output is then passed through the softmax nonlinearity to create probability distribution of the prediction\n",
    "    # The output of this stage is (batch_size, vocab_size)\n",
    "    l_out = lasagne.layers.DenseLayer(l_forward_slice, num_units=vocab_size, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Theano tensor for the targets\n",
    "    target_values = T.ivector('target_output')\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "    cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()\n",
    "\n",
    "    # Retrieve all parameters from the network\n",
    "    #params = pickle.load(open(\"params\",\"rb\"))\n",
    "    #lasagne.layers.set_all_param_values(l_out,params)\n",
    "    all_params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "\n",
    "    # Compute AdaGrad updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "    # In order to generate text from the network, we need the probability distribution of the next character given\n",
    "    # the state of the network and the input (a seed).\n",
    "    # In order to produce the probability distribution of the prediction, we compile a function called probs. \n",
    "    \n",
    "    probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)\n",
    "\n",
    "    # The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    # The phrase is set using the variable generation_phrase.\n",
    "    # The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "    def try_it_out(N=600):\n",
    "        '''\n",
    "        This function uses the user-provided string \"generation_phrase\" and current state of the RNN generate text.\n",
    "        The function works in three steps:\n",
    "        1. It converts the string set in \"generation_phrase\" (which must be over SEQ_LENGTH characters long) \n",
    "           to encoded format. We use the gen_data function for this. By providing the string and asking for a single batch,\n",
    "           we are converting the first SEQ_LENGTH characters into encoded form. \n",
    "        2. We then use the LSTM to predict the next character and store it in a (dynamic) list sample_ix. This is done by using the 'probs'\n",
    "           function which was compiled above. Simply put, given the output, we compute the probabilities of the target and pick the one \n",
    "           with the highest predicted probability. \n",
    "        3. Once this character has been predicted, we construct a new sequence using all but first characters of the \n",
    "           provided string and the predicted character. This sequence is then used to generate yet another character.\n",
    "           This process continues for \"N\" characters. \n",
    "        To make this clear, let us again look at a concrete example. \n",
    "        Assume that SEQ_LENGTH = 5 and generation_phrase = \"The quick brown fox jumps\". \n",
    "        We initially encode the first 5 characters ('T','h','e',' ','q'). The next character is then predicted (as explained in step 2). \n",
    "        Assume that this character was 'J'. We then construct a new sequence using the last 4 (=SEQ_LENGTH-1) characters of the previous\n",
    "        sequence ('h','e',' ','q') , and the predicted letter 'J'. This new sequence is then used to compute the next character and \n",
    "        the process continues.\n",
    "        '''\n",
    "\n",
    "        assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "        sample_ix = []\n",
    "        x,_ = gen_data(len(generation_phrase)-SEQ_LENGTH, 1, generation_phrase,0)\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            ix = np.argmax(probs(x).ravel())\n",
    "            # Alternatively, to sample from the distribution instead:\n",
    "            # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "            sample_ix.append(ix)\n",
    "            x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "            x[:,SEQ_LENGTH-1,:] = 0\n",
    "            x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "\n",
    "        random_snippet = generation_phrase + ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "        print(\"----\\n %s \\n----\" % random_snippet)\n",
    "        fname=open('Output/output_lstm_output.txt','a')\n",
    "        fname.write(\"----\\n %s \\n----\" % random_snippet)\n",
    "        fname.close()\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    print(\"Seed used for text generation is: \" + generation_phrase)\n",
    "    p = 0\n",
    "    try:\n",
    "        for it in xrange(data_size * num_epochs / BATCH_SIZE):\n",
    "            try_it_out(N=600) # Generate text using the p^th character as the start. \n",
    "            \n",
    "            avg_cost = 0;\n",
    "            for _ in range(PRINT_FREQ):\n",
    "                x,y = gen_data(p)\n",
    "                \n",
    "                #print(p)\n",
    "                p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "                if(p+BATCH_SIZE+SEQ_LENGTH >= data_size):\n",
    "                    print('Carriage Return')\n",
    "                    p = 0;\n",
    "                \n",
    "\n",
    "                avg_cost += train(x, y)\n",
    "            all_param = lasagne.layers.get_all_param_values(l_out)\n",
    "            pickle.dump(all_param,open(\"params\",'wb'))\n",
    "            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname=open('Output/output_lstm_loss.txt','a')\n",
    "            fname.write(\"\\n{},{}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname.close()\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Seed used for text generation is: g2B2|d2d2 d2e2|d2cB g2fg|a2A2 A2\n",
      "GA|B2B2 cBAG|A2B2 g2fg|a2gf g2fe|d2B2 B2:|\n",
      "<end>\n",
      "<start>\n",
      "\n",
      "----\n",
      " g2B2|d2d2 d2e2|d2cB g2fg|a2A2 A2\n",
      "GA|B2B2 cBAG|A2B2 g2fg|a2gf g2fe|d2B2 B2:|\n",
      "<end>\n",
      "<start>\n",
      "~t~6666R{]ccccc?*=**!!\u0001uuu5?7ekkk~kExXXMXMX!XXX!X!!bb!bbpp__F\"\"\"x*$^^^^^ggggppgp((U.&&..!:!GGGGGGGGGGGGGGGGaDGDDD**D***\u0001\u0001@\u0001u\tu\tu\n",
      "\n",
      "\n",
      "\n",
      "ww()$$EEEc9!!!!5555522222(ZZNNNXNNNo33Y3Yjk;kkkkXXX++++XXXX>X+//$$$}000$ccc=0=00d'RRBBIIH6L\u0001\u0001\u0001..++//i{{{{{NNNNNNN$hNh;;HHkk{kXXXKKK!\u0001\u0001///;N{{{0$$$H00;ss<<gqgqQGk(kO,1:{:OO****\u0001\u0001\u0001uulll>>>>>>>>>+>+wwwww&$+++^^^^gggggpg~pRVV{{{1iiiccc{00!!!GGGGGGGGGGGGGGGaaaa,222XXXXXXX8XX!X$$$!!!5===2222ZZZZZNNmNNNNNY3YYYYhjkjkkkkkkXXXXXX++XXXXX++$$X$n$0cccc====MMVZbRR#M=bbbpp[[[k*KKS*K^\u0001^^CCCCuuwwwww'''+%%%%%222222222EEE}X!!bbbbbpp__FF\"E**$$m&&&&&(((Fx&X&.+QQiiixx>>>>>G+++aaJJwww{' \n",
      "----"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
