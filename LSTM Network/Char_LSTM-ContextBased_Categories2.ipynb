{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled, CuDNN not available)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasagne Seed for Reproducibility\n",
    "lasagne.random.set_rng(np.random.RandomState(1))\n",
    "\n",
    "# Sequence Length\n",
    "SEQ_LENGTH = 64\n",
    "\n",
    "# Number of units in the two hidden (LSTM) layers\n",
    "N_HIDDEN = 512\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 10\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [\"Data/irish_music.txt\", \"Data/swedish_music.txt\"]\n",
    "context = [\"irish\",\"swedish\"]\n",
    "in_text=[0]*len(files)\n",
    "for i in range(0,len(files)) :\n",
    "    try:\n",
    "        #You can also use your own file\n",
    "        #The file must be a simple text file.\n",
    "        #Simply edit the file name below and uncomment the line.\n",
    "        #in_text = open('data.txt').read() #nietzsche text\n",
    "        in_text[i] = open(files[i], 'r').read() #Music file\n",
    "        in_text[i]= in_text[i].replace('  ','')\n",
    "        in_text[i]= in_text[i].replace('\\r','')\n",
    "        in_text[i]=''.join([j if ord(j) < 128 else ' ' for j in in_text[i]])\n",
    "        in_text[i] = in_text[i].decode(\"utf-8-sig\").encode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Please verify the location of the input file/URL.\")\n",
    "        print(\"A sample txt file can be downloaded from https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "        raise IOError('Unable to Read Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generation_phrase = \"g2B2|d2d2 d2e2|d2cB g2fg|a2A2 A2\\nGA|B2B2 cBAG|A2B2 g2fg|a2gf g2fe|d2B2 B2:|\\n<end>\\n<start>\\n\"\n",
    "#generation_phrase=\"The quick brown fox jumps\"\n",
    "generation_phrases=[]\n",
    "generation_phrases.append(\"a2 ba ge | d2 ed cB | c2 cd Bc | A4 AB |\\ncB cd ef | g4 fe | d2 dB AG | E4 Bd |\\ne2 ge dB | A2 B/c/d D2 | E2 GA/B/ A>G | G4 || \\n<end>\\n<start>\\n\")\n",
    "generation_phrases.append(\"|:A2df a2a2 g4|A2^ce gage f3f|fafd ege^c d3e|f2ed ^c2ec A4|\\nA2df a2a2 g4|A2^ce gage f3f|fafd ege^c d3e|f2ed ^c2ec d4:|\\n<end>\\n<start>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=set()\n",
    "for text in in_text:\n",
    "        a = a|set(text)\n",
    "chars = list(a)\n",
    "chars.sort()\n",
    "data_size, vocab_size = sum(len(text) for text in in_text), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "vocab_size = vocab_size+2\n",
    "n = len(chars)\n",
    "char_to_ix['irish']=n\n",
    "char_to_ix['swedish']=n+1\n",
    "ix_to_char[n]='irish'\n",
    "ix_to_char[n+1]='swedish' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(p, data,context, batch_size = BATCH_SIZE, return_target=True):\n",
    "    '''\n",
    "    This function produces a semi-redundant batch of training samples from the location 'p' in the provided string (data).\n",
    "    For instance, assuming SEQ_LENGTH = 5 and p=0, the function would create batches of \n",
    "    5 characters of the string (starting from the 0th character and stepping by 1 for each semi-redundant batch)\n",
    "    as the input and the next character as the target.\n",
    "    To make this clear, let us look at a concrete example. Assume that SEQ_LENGTH = 5, p = 0 and BATCH_SIZE = 2\n",
    "    If the input string was \"The quick brown fox jumps over the lazy dog.\",\n",
    "    For the first data point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'T','h','e',' ','q'\n",
    "    y (the targets of the neural network) would be the encoding of 'u'\n",
    "    For the second point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'h','e',' ','q', 'u'\n",
    "    y (the targets of the neural network) would be the encoding of 'i'\n",
    "    The data points are then stacked (into a three-dimensional tensor of size (batch_size,SEQ_LENGTH,vocab_size))\n",
    "    and returned. \n",
    "    Notice that there is overlap of characters between the batches (hence the name, semi-redundant batch).\n",
    "    '''\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "            x[n,i,char_to_ix[context]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "   \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, None, vocab_size))\n",
    "\n",
    "    # We now build the LSTM layer which takes l_in as the input layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "\n",
    "    l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    l_forward_1_drop = lasagne.layers.DropoutLayer(l_forward_1, p=0.1)\n",
    "    \n",
    "    l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "        l_forward_1_drop, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    # The l_forward layer creates an output of dimension (batch_size, SEQ_LENGTH, N_HIDDEN)\n",
    "    # Since we are only interested in the final prediction, we isolate that quantity and feed it to the next layer. \n",
    "    # The output of the sliced layer will then be of size (batch_size, N_HIDDEN)\n",
    "    l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)\n",
    "\n",
    "    # The sliced output is then passed through the softmax nonlinearity to create probability distribution of the prediction\n",
    "    # The output of this stage is (batch_size, vocab_size)\n",
    "    l_out = lasagne.layers.DenseLayer(l_forward_slice, num_units=vocab_size, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Theano tensor for the targets\n",
    "    target_values = T.ivector('target_output')\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "    cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()\n",
    "\n",
    "    # Retrieve all parameters from the network\n",
    "    params = pickle.load(open(\"params_context_country_dropout\",\"rb\"))\n",
    "    lasagne.layers.set_all_param_values(l_out,params)\n",
    "    all_params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "\n",
    "    # Compute AdaGrad updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "    # In order to generate text from the network, we need the probability distribution of the next character given\n",
    "    # the state of the network and the input (a seed).\n",
    "    # In order to produce the probability distribution of the prediction, we compile a function called probs. \n",
    "    \n",
    "    probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)\n",
    "\n",
    "    # The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    # The phrase is set using the variable generation_phrase.\n",
    "    # The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "    def try_it_out(context, generation_phrase, N=600):\n",
    "        '''\n",
    "        This function uses the user-provided string \"generation_phrase\" and current state of the RNN generate text.\n",
    "        The function works in three steps:\n",
    "        1. It converts the string set in \"generation_phrase\" (which must be over SEQ_LENGTH characters long) \n",
    "           to encoded format. We use the gen_data function for this. By providing the string and asking for a single batch,\n",
    "           we are converting the first SEQ_LENGTH characters into encoded form. \n",
    "        2. We then use the LSTM to predict the next character and store it in a (dynamic) list sample_ix. This is done by using the 'probs'\n",
    "           function which was compiled above. Simply put, given the output, we compute the probabilities of the target and pick the one \n",
    "           with the highest predicted probability. \n",
    "        3. Once this character has been predicted, we construct a new sequence using all but first characters of the \n",
    "           provided string and the predicted character. This sequence is then used to generate yet another character.\n",
    "           This process continues for \"N\" characters. \n",
    "        To make this clear, let us again look at a concrete example. \n",
    "        Assume that SEQ_LENGTH = 5 and generation_phrase = \"The quick brown fox jumps\". \n",
    "        We initially encode the first 5 characters ('T','h','e',' ','q'). The next character is then predicted (as explained in step 2). \n",
    "        Assume that this character was 'J'. We then construct a new sequence using the last 4 (=SEQ_LENGTH-1) characters of the previous\n",
    "        sequence ('h','e',' ','q') , and the predicted letter 'J'. This new sequence is then used to compute the next character and \n",
    "        the process continues.\n",
    "        '''\n",
    "\n",
    "        assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "        sample_ix = []\n",
    "        x,_ = gen_data(len(generation_phrase)-SEQ_LENGTH,generation_phrase,context,1,0)\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            ix = np.argmax(probs(x).ravel())\n",
    "            # Alternatively, to sample from the distribution instead:\n",
    "            # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "            sample_ix.append(ix)\n",
    "            x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "            x[:,SEQ_LENGTH-1,:] = 0\n",
    "            x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "\n",
    "        random_snippet = ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "        print(\"----\\n %s \\n %s \\n----\" % (context,random_snippet))\n",
    "        fname=open('Output/country_lstm_output_context_dropout_'+context+'.txt','a')\n",
    "        fname.write(\"----\\n<start>\\n%s\\n----\" % (random_snippet))\n",
    "        fname.close()\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    #print(\"Seed used for text generation is: \" + generation_phrase)\n",
    "    p = 0\n",
    "    try:\n",
    "        pos=0\n",
    "        positions=[0]*len(files)\n",
    "        for it in xrange(data_size * num_epochs / BATCH_SIZE):\n",
    "            try_it_out(context[0],generation_phrases[0],N=600)\n",
    "            try_it_out(context[1],generation_phrases[1],N=600) # Generate text using the p^th character as the start. \n",
    "            \n",
    "            avg_cost = 0;\n",
    "            for _ in range(PRINT_FREQ):\n",
    "                p=positions[pos]\n",
    "                x,y = gen_data(p,in_text[pos],context[pos])\n",
    "                \n",
    "                #print(p)\n",
    "                p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "                if(p+BATCH_SIZE+SEQ_LENGTH >= len(in_text[pos])):\n",
    "                    print('Carriage Return')\n",
    "                    p = 0;\n",
    "                \n",
    "                #print(\"context is \",context[pos],\"positions is \", p)\n",
    "                avg_cost += train(x, y)\n",
    "                positions[pos]=p\n",
    "                pos = (pos+1)%len(files)\n",
    "            all_param = lasagne.layers.get_all_param_values(l_out)\n",
    "            pickle.dump(all_param,open(\"params_context_country_dropout_web\",'wb'))\n",
    "            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname=open('Output/country_lstm_loss_context_dropout_web.txt','a')\n",
    "            fname.write(\"\\n{},{}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            fname.close()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "----\n",
      " irish \n",
      " X:21\n",
      "T:Shiving varsin, The\n",
      "R:reel\n",
      "H:Sem #96\n",
      "Z:id:hn-sp-14\n",
      "M:3|\n",
      "L:1/8\n",
      "K:A\n",
      "gf a>g | a2 fd d/ | BB B/c/d B/B/ | cB A/B/ :|\n",
      "|: g2 | gafd d2 (3 Bd | g2 g/ g/f/d/B/ | B/A/G/A/ B/B/G/ G/|\n",
      "{B}B/B/ cB BA | B/G/G FG ED | (3ABc BA G2 | BA GF G/|\n",
      "GE | G4 | GF G> | B2 DG/F/ d/-| E>G | GE EC |/G/A/B/c/ G GA/B/||\n",
      "<end>\n",
      "<start>\n",
      "X:2\n",
      "T:Villing Parlich Rogle\n",
      "R:polska J\n",
      "A:Semlel #10\n",
      "Z:id:hn-horka-14\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:D\n",
      "Ad |: (3c/A/^c/ (3A (3B)(|d ((3B)B (3dBc | (AB)^cd (3^fg) (ag) (3efg |\n",
      "(ba)g (fe)(fd) (eg)ee | (fe)(ce) (ga)) (3f)d | (agf) (fd).(e.. (5g ag ag | (3fg^f (3def g2f2 |\n",
      "ed^c d2 (3B^G^| (3d^c d2 | e^f g2 | \n",
      "----\n",
      "----\n",
      " swedish \n",
      " X:8\n",
      "T:Humoung Roll ann\n",
      "R:polska Skera ky Jam (18., H\\\"ods\"on Jion (1878-1905, #176\n",
      "Z:id:hn-sp-210\n",
      "M:3/4\n",
      "L:1/16\n",
      "K:D\n",
      "DF | F2 d2 d2 | B2 G2 GB | A4 D2 | F3 D3 D |\n",
      "FE FE DD | D2 BA AG | B4 | GA B2 | G2 G2 | B2 B3 A^G|\n",
      "G4 B2 | Bd B2 d2 | B2 G2 BG | A2 G2 :|\n",
      "|: B2 Bd g2 | g4 g2 | B2 d2 dB | d4 B2 | B2 B2 d2 | G4 B2 |\n",
      "BA BA dB | g2 BA B2 | dB GA GB | AB GF GB | A2 G2 | \n",
      "d2 ed | B/ B/c/d/ | e2 GB |\n",
      "A2 B2 dc | B/ B/c/d e2 | d4 | B2 B2 A2 | d2 G2 B2 | A3 E2 ||\n",
      "B3 Bd c|Bd g2 fd | B2 Bd Bd | B4 G4 |\n",
      "g2 B2 BG | G2 G2 BG | A2 B2 cd | e2 d2 B2 | G2 B2 d2 |\n",
      "g2 gg b2 | ag fe dc | B4 Bg ga | b2 ba gf | g4 d2 |\n",
      " \n",
      "----"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
